{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/DeepLearnHackathon/blob/main/ParticleImagesChallenge/ParticleImages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Particle Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Introduction:**\n",
        "\n",
        "Machine Learning algorithms have become an increasingly important tool for analyzing the data from the Large Hadron Collider (LHC). Identification of particles in LHC collisions is an important task of LHC detector reconstruction algorithms.\n",
        "\n",
        "Here we present a challenge where one of the detectors (the Electromagnetic Calorimeter or ECAL) is used as a camera to analyze detector images from two types of particles: electrons and photons that deposit their energy in this detector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dataset:**\n",
        "\n",
        "Each pixel in the image corresponds to a detector cell, while the intensity of the pixel corresponds to how much energy is measured in that cell. Timing of the energy deposits are also available, though this may or may not be relevant. The dataset contains 32x32 Images of the energy hits and their timing (channel 1: hit energy and channel 2: its timing) in each calorimeter cell (one cell = one pixel) for the two classes of particles: Electrons and Photons. The dataset contains around four hundred thousand images for electrons and photons. Please note that your final model will be evaluated on an unseen test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverables\n",
        "\n",
        "* `.ipynb` (and a PDF version of it with outputs showing your results) file showing your solution, including your study of the data, final model structure, hyperparameters and the wat the model was trained that yielded the best possible performance.\n",
        "* Final model accuracy (training and validation) ROC curve and AUC score, as well as an additional plot (e.g. precision-recall curves, confusion matrix) which further showcases the performance of your model.\n",
        "* Your trained model containing the model architecture and its trained weights (HDF5 file, .pb file, .pt file, etc.). Also show in your notebooks how to load and use your model.\n",
        "\n",
        "**Note: You are free to use the ML framework of your choice.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtRyfNv9XVn"
      },
      "source": [
        "## Download the Dataset\n",
        "\n",
        "If you are working in Colab, to not have to re-download the data all the time, you can mount your Google Drive and download/fetch the data to/from there ([link for more info](https://towardsdatascience.com/different-ways-to-connect-google-drive-to-a-google-colab-notebook-pt-1-de03433d2f7a))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcK1wY4Qt_7Y"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "!wget https://cernbox.cern.ch/s/9YELv279NIAE9B8/download -O /content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\n",
        "!wget https://cernbox.cern.ch/s/EahdXxzgq7nPodp/download -O /content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"/content/\" # Put here in what directory your data lives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BepRE7pn8Du7"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnLzC5paz0hb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import h5py\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwc56kXJ8TLo"
      },
      "source": [
        "## Loading Image Data\n",
        "- Two classes of particles: electrons and photons\n",
        "- 32x32 matrices (two channels - hit energy and time) for the two classes of particles electrons and photons impinging on a calorimeter (one calorimetric cell = one pixel).\n",
        "- Note that although timing channel is provided, it may not necessarily help the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr4QIMlt424u"
      },
      "outputs": [],
      "source": [
        "# 1 -> electron\n",
        "filename = os.path.join(data_dir, \"SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\")\n",
        "data1 = h5py.File(filename, \"r\")\n",
        "Y1 = data1[\"y\"]\n",
        "X1 = data1[\"X\"]\n",
        "\n",
        "# 0 -> photon\n",
        "filename = os.path.join(data_dir, \"SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\")\n",
        "data0 = h5py.File(filename, \"r\")\n",
        "Y0 = data0[\"y\"]\n",
        "X0 = data0[\"X\"] \n",
        "\n",
        "# Combining datasets into one mixed dataset\n",
        "X_final = np.concatenate((X0[:], X1[:]), axis=0)\n",
        "Y_final = np.concatenate((Y0[:], Y1[:]), axis=0)\n",
        "\n",
        "num_imgs = Y_final.shape[0]\n",
        "print(\"Number of images: {}\".format(num_imgs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JpHCOf38fDL"
      },
      "source": [
        "# Configure Training / Validation / Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RTXS58x46Fq"
      },
      "outputs": [],
      "source": [
        "# Divide into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split( \n",
        "    X_final,          \n",
        "    Y_final,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Further divide test into test and validation\n",
        "X_valid, X_test, y_valid, y_test = train_test_split( \n",
        "    X_test,\n",
        "    y_test,\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Colab has limited RAM, so you might need to clear some memory...\n",
        "del(Y1, X1, Y0, X0, X_final, Y_final) \n",
        "\n",
        "print(f\"X_train shape: {X_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape} - y_test shape: {y_test.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape} - y_valid shape: {y_valid.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1:\n",
        "\n",
        "*Data: Training data*\n",
        "\n",
        "Explore, visualize and analyze the data found in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2:\n",
        "\n",
        "*Data: Training and validation data*\n",
        "\n",
        "Train a model by fitting it to the training data. Use at least one metric such as `roc_auc_score`, `accuracy`, etc. to analyze the model's performance on the validation data. Using that performance metric, optimize or improve your model. It should be clear from your notebook how you perform this optimization and you should explain your thinking clearly.\n",
        "\n",
        "As you work on your model, you may use a subset of the actual dataset to haisten your tests. However, for final submission, you must use the full test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use your framework of choice. \n",
        "\n",
        "# Define your model here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train your model here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: \n",
        "\n",
        "*Data: Testing data*\n",
        "\n",
        "Without having done any optimization using the testing data set, analyze the performance of the model on the testing data. Your analysis should include the AUC score, a ROC curve plot, and at least one other plot of your choice such as precision-recall curves, confusion matrix, etc. Try to get your model to perform with AUC > 90%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
